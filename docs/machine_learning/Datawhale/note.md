# Datawhale  X 李宏毅苹果书 AI夏令营

——机器学习初体验

## 机器学习基础

机器学习, 学习的究竟是什么东西呢? 或者说, 让机器拥有学习什么的能力呢? 简而言之, ***机器学习就是让机器具备寻找一个函数的能力***. 例如, 我们需要判断这张照片里面有没有猫, 相当于是将一张照片输入进人脑, 最后输出的就是一个数字: 1代表有, 0代表没有. 那么机器能不能通过学习获得这个本领吗? 答案是可以的. 那么这训练过程其实就是训练了一个函数, 输入的是一张照片, 输出的是一个数字, 用来判断照片里面有没有猫. 再举一个例子, AlphaGo其实也是一个函数, 输入的是目前为止双方的落子, 输出是下一步棋应该落在的位置. 或者more specifically, 应该是下一步棋在每一个空位置的获胜可能性, 然后选择可能性最大的.

当然, 函数的类型也有可能变换, 而***随着寻找函数的不同, 机器学习也分为不同的类别***. *假如说函数的输出是一个数值, 一个标量(scalar), 那么机器学习的任务就称为**回归**(regression)*. 举一个例子: 机器要预测未来一个时间的PM2.5值, 这个函数输入的是种种和PM2.5值有关的指数, 例如说今天的PM2.5值, 平均温度, 平均的臭氧浓度等, 输出的就是明天中午的PM2.5值.

另外一个常见的任务是***分类***(classification). 人类先准备好一些选项, 这些选项称为类别, 那么函数的输出就是从设定的选项中选择一个当作输出. 例如可以训练一个模型, 判断照片中存在的动物是猫, 狗, 仓鼠, 还是其他.另外, 除了回归和分类, 还有***结构化学习***(structured learning). 机器不是输出一个数字或者是选择一个选项, 而是产生一个有结构的物体, 例如机器写一篇文章.

那么机器是如何学习一个函数的呢? 

第一步: 写出一个带有位置参数的函数, 希望它能够用来实现我所有希望实现的功能, 例如:
$$
y = b + wx
$$
那么w b两个参数都是未知的, 然后x是输入的变量, 最后输出的是一个整数, 因此是一个简单的回归. 上面这个带有未知**参数(parameter)**的函数就称为**模型(model)**. 而这个x就称为**特征(feature)**, w称为**权重(weight)**, b称为**偏置(bias)**.

第二步: ***定义损失(loss)***. 这个损失其实也是一个函数, 因此常称之为损失函数, 这个损失函数的意义就是用来衡量机器目前选择的权重和偏置有多合理. 因此损失函数为: 
$$
L(b,w)
$$
很明显这是一个多变量的函数. 在实战中, 假如说训练出来一个权重一个偏置, 那么就会用**领域知识(domain knowledge)**实际的instance来带入: 例如这里就是输入真实的x, 然后比较一下输出的y和真实的y. 如何比较呢? 常见的有: 
$$
e = |y - \hat{y}|\hspace{1cm}平均绝对误差(MAE)
$$

$$
e = (\hat{y} - y)^2\hspace{1cm}均方误差(MSE)
$$

在有些任务中, y是概率分布, 那么这个时候可能会选择***交叉熵(cross entropy loss)***. 

在之前举的例子中, 不同的权重和偏置根据领域知识的输入会得到不同的输出, 我们可以画一个图来记录下w b所对应的Loss, 如下: 

![image](img/1.png)

在这张图中, 画出来的等高线称为**误差表面(error surface)**, 在这条等高线上, 处处的loss都是相同的. 那么有了loss来衡量目前权重和偏置的"质量如何", 那么如何更新这些参数呢? 

附: 实战中, "领域知识的输入"具体来说是在训练的时候, 输入训练集的输入, 得到对应的结果, 然后根据训练集的ground truth, 以loss的方式进行监督训练 

第三步: 解一个最优化的问题. 为了更新参数, 我们需要对偏置和权重进行加加减减, 那么依据是什么呢? 肯定需要利用上之前算出来的Loss函数. 我们经常使用**梯度下降(gradient descent)**来进行优化. 首先我们考虑首先更新权重, 而不是偏置. 如果bias视为常数, 那么损失函数将会是关于w的函数, 那么我们可以画出$L-W$的函数图像. 在目前的w下, 会有对应的斜率,  我们需要根据这个斜率来更新w的数值. 我们使用公式如下: 
$$
w^1 \leftarrow w^0 - \eta \frac{\partial L}{\partial W}\Bigg|_{w=w^0}
$$
在图中的直观表示就是: 

![image](img/2.png)

在上述的公式中, $\eta$是一个自己设置的参数, 叫做**学习率**(learning rate). 这不是机器自己找出来的, 因此要和被称为**超参数(hyperparameter)**. 那么一步一步, 我们希望w能够取中低点. 但是在上面这个图中, 我们看到: 有局部最小值和全局最小值. 如果先到达了局部最小值, 那么斜率几乎为零, 步长几乎为零, 这不就是意味着w的取值永远停在了局部最小值的阶段吗? 但是事实上局部最小值是一个假问题, 因为损失函数我们会故意设计成**凸函数**, 从而使得损失函数空间没有弯曲, 局部最低点就是全局最低点. 在两个参数的情况下使用梯度下降, 其实和之前一个参数没有什么不同, 因此可以推广bias参数的更新. 在Pytorch中, 梯度回传会自动帮忙计算.

## 线性模型

### 基本概念原理

对于上面这个例子来说，使用的函数是： $$y = kx + b$$用来拟合数据；那么我能不能考虑更复杂的情况，即设计更复杂的函数呢？当然是可以的，但是设计上有大学问：

假如说我设计：$y = \sum_{i=1}^{t}k_i+\sum_{i=1}^{t}b_i$, 相当于是$\overline{y}=\sum_{i=1}^{t}y_i$，那么这个函数真的能拟合到更多的信息吗？事实上是不能的，因为事实上求导公式依然是$w^1 \leftarrow w^0 - \eta \frac{\partial L}{\partial W}\Bigg|_{w=w^0}$，那么站在gradient descent的角度上，根本就是换汤不换药。这就是所谓的**“没有引入非线性性”**，而“把输入的特征x乘上一个权重，再加上一个偏置就得到预测效果”的模型称为**线性模型（linear model）**。线性模型有很大的限制，这一种来自于模型的限制称为模型的偏差，无法模拟真实的情况。

那么如何引入非线性性呢？不妨从简单的角度进行思考：能不能在线性模型上引入非线性性呢？因此我们可以考虑分段线性曲线。不妨考虑这样一种非线性函数：当 x 轴的值小于某一个阈值（某个定值）的时候，大于另外一个定值阈值的时候，中间有一个斜坡；所以它是先水平的，再斜坡，再水平的。更具体来说，这种函数叫做hard sigmoid函数，深度学习中pytorch具体定义如下：
$$
\text{Hardsigmoid}(x) = 
\begin{cases} 
0 & \text{if } x \leq -3, \\ 
1 & \text{if } x \geq +3, \\ 
\frac{x}{6} + \frac{1}{2} & \text{otherwise}
\end{cases}
$$
这其实很像***ReLU(Rectified Linear Unit)***的变体。那么通过下面这个图，可以直观的看出：为什么一堆这种非线性函数可以拟合各种各样的函数：

<img src="img/3.png" alt="image" style="zoom: 67%;" />

分段线性曲线就可以逼近这一个连续的曲线，就可以逼近有角度的、有弧度的这一条曲线。 所以可以用分段线性曲线去逼近任何的连续的曲线，而每个分段线性曲线都可以用一大堆蓝色的函数组合起来。也就是说，***只要有足够的蓝色函数把它加起来，就可以变成任何连续的曲线***。

当然，可以使用的非线性性函数不止分段线性函数。比如sigmoid：
$$
y=c\dfrac{1}{1+e^{-(b+wx_1)}}\hspace{1cm}分式版本\\
y=c\sigma(b+wx_1)\hspace{1cm}简洁版本
$$
它的图像和hard sigmoid的图像如下，能够直观看到，为什么hard sigmoid称为"hard"了：

<img src="img/4.png" alt="image" style="zoom:67%;" />

这里利用b w c参数可以控制sigmoid函数的形状。其中，w可以控制“斜率”，b可以控制偏移量，c可以控制高度。具体效果如下： 

<img src="img/5.png" alt="image" style="zoom:50%;" />

可想而知，把各种不一样的sigmoid函数叠加起来，就能够去逼近各种不同的分段线性函数，而分段线性函数可用来近似各种不同的连续函数。所以换而言之，不同的sigmoid函数叠加起来可以拟合各种连续的函数。

### 矩阵表示

假如说用三个hard sigmoid函数来进行拟合：

<img src="img/6.png" alt="image" style="zoom:50%;" />

那么换一个简单的scenario：使用三个sigmoid function进行拟合。考虑三个数据喂进去： $$(x_i,y_i)\hspace{0.3cm}i=1,2,3$$, 那么我就希望能够找到合适的$$b_i$$和$$w_{ij} $$（代表第i个sigmoid里面乘给第j个特征的权重）。首先对数据进行权重分配（或者说，对特征分配权重，这时候都还是线性操作）
$$
r_1 = b_1 +w_{11}x_1 + w_{12}x_2 + w_{13}x_3\\
r_2 = b_2 +w_{21}x_1 + w_{22}x_2 + w_{23}x_3\\
r_3 = b_3 +w_{31}x_1 + w_{32}x_2 + w_{33}x_3\\
$$
那么这个时候矩阵就排得上用场了：
$$
\begin{bmatrix}
r_1 \\
r_2 \\
r_3
\end{bmatrix}
= 
\begin{bmatrix}
w_{11} & w_{12} & w_{13} \\
w_{21} & w_{22} & w_{23} \\
w_{31} & w_{32} & w_{33}
\end{bmatrix}
\begin{bmatrix}
x_1 \\
x_2 \\
x_3
\end{bmatrix}
+
\begin{bmatrix}
b_1 \\
b_2 \\
b_3
\end{bmatrix}
$$
在这里：$r = b + Wx$, 之后经过sigmoid函数以引入非线性性。（这一步又称为：经过激活函数），表示为：$a = \sigma(r)$

<img src="img/7.png" alt="image" style="zoom:50%;" />

引入了非线性性，最后的三个非线性函数也应该又权重分配，因此引入线性性分配三个非线性性函数的权重，并配上偏置：$y=b+\textbf{c}^{T}\textbf{a}$

<img src="img/8.png" alt="image" style="zoom:50%;" />

### 定义损失、优化、梯度更新

